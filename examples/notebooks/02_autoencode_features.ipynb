{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import dill as pickle\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import yaml\n",
    "from utils import load_yaml\n",
    "import os\n",
    "from basic_utils import *\n",
    "os.chdir(\"../\")\n",
    "\n",
    "model_name='all-MiniLM-L6-v2'\n",
    "model_string_encoder = SentenceTransformer(model_name)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_strings(df):\n",
    "    x = model_string_encoder.encode(df.values, show_progress_bar=True)\n",
    "    return x\n",
    "\n",
    "engine = sqlalchemy.create_engine(\"mariadb+mariadbconnector://guest:relational@relational.fit.cvut.cz:3306/financial\")\n",
    "\n",
    "trans = pd.read_sql_table(\"trans\", engine)\n",
    "loan = pd.read_sql_table(\"loan\", engine)\n",
    "order = pd.read_sql_table(\"order\", engine)\n",
    "card = pd.read_sql_table(\"card\", engine)\n",
    "account = pd.read_sql_table(\"account\", engine)\n",
    "client = pd.read_sql_table(\"client\", engine)\n",
    "disp = pd.read_sql_table(\"disp\", engine)\n",
    "district = pd.read_sql_table(\"district\", engine)\n",
    "\n",
    "MAX_SIZE_PER_TABLE = 1_000\n",
    "THRESHOLD_RATIO_CATEGORIES = 0.2\n",
    "THRESHOLD_ABSOLUTE_CATEGORIES = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.to_parquet(\"./data/trans.parquet\")\n",
    "loan.to_parquet(\"./data/loan.parquet\")\n",
    "order.to_parquet(\"./data/order.parquet\")\n",
    "card.to_parquet(\"./data/card.parquet\")\n",
    "account.to_parquet(\"./data/account.parquet\")\n",
    "client.to_parquet(\"./data/client.parquet\")\n",
    "disp.to_parquet(\"./data/disp.parquet\")\n",
    "district.to_parquet(\"./data/district.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_enc(df):\n",
    "    mapping = dict()\n",
    "    for i, _ in enumerate(df.columns):\n",
    "        if df.dtypes.iloc[i] == \"float64\":\n",
    "            mapping[str(df.columns[i])] = \"proc_raw\"\n",
    "        elif df.dtypes.iloc[i] == \"object\":\n",
    "            if (len(df[df.columns[i]].unique()) / len(df) <= THRESHOLD_RATIO_CATEGORIES) & \\\n",
    "                (len(df[df.columns[i]].unique()) <= THRESHOLD_ABSOLUTE_CATEGORIES):\n",
    "                mapping[str(df.columns[i])] = \"proc_objects_one_hot\"\n",
    "            else:\n",
    "                mapping[str(df.columns[i])] = \"proc_objects_string\"\n",
    "        elif df.dtypes.iloc[i] == \"int64\":\n",
    "            if (len(df[df.columns[i]].unique()) / len(df) <= THRESHOLD_RATIO_CATEGORIES) & \\\n",
    "                (len(df[df.columns[i]].unique()) <= THRESHOLD_ABSOLUTE_CATEGORIES):\n",
    "                mapping[str(df.columns[i])] = \"proc_objects_one_hot\"\n",
    "            else:\n",
    "                mapping[str(df.columns[i])] = \"proc_raw\"\n",
    "        elif df.dtypes.iloc[i] == \"datetime64[ns]\":\n",
    "            #features.append( proc_datetime( df[df.columns[i]] ) )\n",
    "            mapping[str(df.columns[i])] = \"proc_datetime\"\n",
    "        else:\n",
    "            print(\"WARNING: UNKNOWN COLUMN TYPE\")\n",
    "        \n",
    "    return mapping\n",
    "\n",
    "\n",
    "def auto_edgerizer(dfs):\n",
    "    strong_edges = list()\n",
    "    all_maps = dict()\n",
    "    for name, df in dfs:\n",
    "        curr_map = dict()\n",
    "        for i in range(len(df.columns)):\n",
    "            if (df.dtypes.iloc[i] == \"object\") | (df.dtypes.iloc[i] == \"int64\"):\n",
    "                curr_map[df.columns[i]] = df[df.columns[i]].unique()\n",
    "        all_maps[str(name)] = copy.copy(curr_map)\n",
    "    \n",
    "    # Now find overlaps\n",
    "    keys = sorted(list(all_maps.keys()))\n",
    "    for k in range(len(keys)):\n",
    "        for l in range(len(keys)):\n",
    "            if k < l:\n",
    "                k_cols = sorted(list(all_maps[keys[k]].keys()))\n",
    "                l_cols = sorted(list(all_maps[keys[l]].keys()))\n",
    "                for kk in range(len(k_cols)):\n",
    "                    for ll in range(len(l_cols)):\n",
    "                        inter = set(all_maps[keys[k]][k_cols[kk]].tolist()).intersection( \n",
    "                            set(all_maps[keys[l]][l_cols[ll]].tolist() ))\n",
    "                        if (len(set(all_maps[keys[k]][k_cols[kk]].tolist() )) == len(inter)) | \\\n",
    "                            (len(set(all_maps[keys[l]][l_cols[ll]].tolist() )) == len(inter)):\n",
    "                            if len(set(all_maps[keys[l]][l_cols[ll]].tolist() )) == len(set(all_maps[keys[k]][k_cols[kk]].tolist() )) and \\\n",
    "                                len(set(all_maps[keys[l]][l_cols[ll]].tolist() )) == len(inter):\n",
    "                                tmp_dict = dict()\n",
    "                                tmp_dict[\"name\"] = str(keys[k]) + \"_\" + str(keys[l]) \n",
    "                                tmp_dict[\"from\"] = str(keys[k])\n",
    "                                tmp_dict[\"to\"] = str(keys[l])\n",
    "                                tmp_dict[\"transform\"] = \"\"\n",
    "                                tmp_dict[\"from_col\"] = str(k_cols[kk])\n",
    "                                tmp_dict[\"to_col\"] = str(l_cols[ll])\n",
    "                                strong_edges.append(tmp_dict)\n",
    "                                \n",
    "    return strong_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'district_id': 'proc_raw',\n",
       " 'A2': 'proc_objects_string',\n",
       " 'A3': 'proc_objects_one_hot',\n",
       " 'A4': 'proc_raw',\n",
       " 'A5': 'proc_raw',\n",
       " 'A6': 'proc_raw',\n",
       " 'A7': 'proc_raw',\n",
       " 'A8': 'proc_objects_one_hot',\n",
       " 'A9': 'proc_objects_one_hot',\n",
       " 'A10': 'proc_raw',\n",
       " 'A11': 'proc_raw',\n",
       " 'A12': 'proc_raw',\n",
       " 'A13': 'proc_raw',\n",
       " 'A14': 'proc_raw',\n",
       " 'A15': 'proc_raw',\n",
       " 'A16': 'proc_raw'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_district = auto_enc(district)\n",
    "features_district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nedges:\\n- name: loan_account\\n  from: loan\\n  to: account\\n  transform: ''\\n  from_col: loan_id\\n  to_col: account_id\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "edges:\n",
    "- name: loan_account\n",
    "  from: loan\n",
    "  to: account\n",
    "  transform: ''\n",
    "  from_col: loan_id\n",
    "  to_col: account_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_key_column(data):\n",
    "    for i in range(len(data.columns)):\n",
    "        if (data.dtypes.iloc[i] == \"object\") or (data.dtypes.iloc[i] == \"int64\"):\n",
    "            if len(data[data.columns[i]].unique()) == len(data):\n",
    "                return str(data.columns[i])\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_auto_config_from_files(files, yaml_file=\"./src/default01.yml\", projectname=\"basic\"):\n",
    "    \"\"\"\n",
    "    Create a yaml file as well as a python file to give a \n",
    "    customizable basis for any following pipeline\n",
    "    \"\"\"\n",
    "    # 0. Read files and create dfs tuple-object\n",
    "    dfs = []\n",
    "    for entry in files:\n",
    "        dfs.append((entry[(entry.rfind(\"/\") + 1) : entry.rfind(\".\")], pd.read_parquet(entry)))\n",
    "\n",
    "    # 1. Create Node-FeatureExtraction Mapping\n",
    "    mappings = list()\n",
    "    for i, tup in enumerate(dfs):\n",
    "        name, df = tup\n",
    "        obj = dict()\n",
    "        obj[\"name\"] = name\n",
    "        obj[\"file\"] = files[i]\n",
    "        obj[\"key\"] = identify_key_column(df)\n",
    "        obj[\"label\"] = identify_key_column(df)\n",
    "        obj[\"features\"] = \"features\"\n",
    "        obj[\"transform\"] = auto_enc(df)\n",
    "        mappings.append(obj)\n",
    "\n",
    "    # 2. Create Edge Mapping\n",
    "    # strong_edges = auto_edgerizer(dfs)\n",
    "\n",
    "    final_dict = dict()\n",
    "    final_dict[\"project\"] = projectname\n",
    "    final_dict[\"data_dir\"] = \"./data\"\n",
    "    final_dict[\"backend\"] = {\"uri\": final_dict[\"data_dir\"] + \"/\" + projectname}\n",
    "    final_dict[\"script\"] = projectname + \"_utils\"\n",
    "    final_dict[\"nodes\"] = mappings\n",
    "    #final_dict[\"edges\"] = strong_edges\n",
    "\n",
    "    # 3. Write yaml file\n",
    "    f = open(yaml_file, \"w\")\n",
    "    yaml.dump(final_dict, f, sort_keys=False)\n",
    "    f.close()\n",
    "    print(\"YAML file saved.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file saved.\n"
     ]
    }
   ],
   "source": [
    "create_auto_config_from_files([\n",
    "        \"./data/trans.parquet\",\n",
    "        \"./data/loan.parquet\",\n",
    "        \"./data/order.parquet\",\n",
    "        \"./data/card.parquet\",\n",
    "        \"./data/account.parquet\",\n",
    "        \"./data/client.parquet\",\n",
    "        \"./data/disp.parquet\",\n",
    "        \"./data/district.parquet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a38b9456cc24db7a9a9b629ac2d168a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_feature_pipeline_from_yaml(file=\"./src/default01.yml\"):\n",
    "    config = load_yaml(file)\n",
    "    module = __import__(config[\"script\"])\n",
    "\n",
    "    pp = getattr(module, \"postproc\")\n",
    "\n",
    "    for entry in config[\"nodes\"]:\n",
    "        data = pd.read_parquet(entry[\"file\"])\n",
    "        feature_data = []\n",
    "        for transform in entry[\"transform\"]:\n",
    "            func = entry[\"transform\"][transform]\n",
    "            f = getattr(module, func)\n",
    "            feature_data.append(f(data[transform]))\n",
    "        \n",
    "        data[entry[\"features\"]] = pp(feature_data).tolist()\n",
    "        data.to_parquet(entry[\"file\"])\n",
    "    \n",
    "    for entry in config[\"edges\"]:\n",
    "        for node in config[\"nodes\"]:\n",
    "            if node[\"name\"] == entry[\"from\"]:\n",
    "                df = pd.read_parquet(node[\"file\"])\n",
    "                # Write parquet files of edges:\n",
    "                df[[entry[\"from_col\"], entry[\"to_col\"]]].to_parquet(\n",
    "                    config[\"data_dir\"] + '/edges_' + entry[\"from\"] + \"_\" + entry[\"to\"] + \".parquet\")\n",
    "\n",
    "run_feature_pipeline_from_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"./data/trans.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Dataframes Schema\n",
    "# 1. Read in the Transformation pipelines etc\n",
    "# 2. Create Batch\n",
    "# 3. Load model checkpoint and evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgnn_demos-1xirZSoL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
