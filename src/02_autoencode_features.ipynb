{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import dill as pickle\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import yaml\n",
    "from utils import load_yaml\n",
    "\n",
    "model_name='all-MiniLM-L6-v2'\n",
    "model_string_encoder = SentenceTransformer(model_name)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_strings(df):\n",
    "    x = model_string_encoder.encode(df.values, show_progress_bar=True)\n",
    "    return x\n",
    "\n",
    "engine = sqlalchemy.create_engine(\"mariadb+mariadbconnector://guest:relational@relational.fit.cvut.cz:3306/financial\")\n",
    "\n",
    "trans = pd.read_sql_table(\"trans\", engine)\n",
    "loan = pd.read_sql_table(\"loan\", engine)\n",
    "order = pd.read_sql_table(\"order\", engine)\n",
    "card = pd.read_sql_table(\"card\", engine)\n",
    "account = pd.read_sql_table(\"account\", engine)\n",
    "client = pd.read_sql_table(\"client\", engine)\n",
    "disp = pd.read_sql_table(\"disp\", engine)\n",
    "district = pd.read_sql_table(\"district\", engine)\n",
    "\n",
    "MAX_SIZE_PER_TABLE = 1_000\n",
    "THRESHOLD_RATIO_CATEGORIES = 0.2\n",
    "THRESHOLD_ABSOLUTE_CATEGORIES = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_transformer(period):\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def cos_transformer(period):\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def proc_raw(data):\n",
    "    return data.values\n",
    "\n",
    "\n",
    "def proc_objects_one_hot(data):\n",
    "    return pd.get_dummies(data).values\n",
    "\n",
    "\n",
    "def proc_objects_string(data):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    "    model_string_encoder = SentenceTransformer(model_name)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_strings(df):\n",
    "        x = model_string_encoder.encode(df.values, show_progress_bar=True)\n",
    "        return x\n",
    "    return encode_strings(data.fillna(''))\n",
    "\n",
    "\n",
    "def postproc(data):\n",
    "    if len(data[-1].shape) == 1:\n",
    "        data[-1] = np.expand_dims(data[-1], 1)\n",
    "    return np.hstack(data)\n",
    "\n",
    "\n",
    "def proc_datetime(data):\n",
    "    feature_tmp_year = data.dt.year\n",
    "    feature_tmp_day = data.dt.day\n",
    "    # Cyclical features:\n",
    "    feature_tmp_month = data.dt.month\n",
    "    feature_tmp_hour = data.dt.hour\n",
    "    feature_tmp_minute = data.dt.minute\n",
    "    feature_tmp_second = data.dt.second\n",
    "    feature_tmp_day_of_week = data.dt.day_of_week\n",
    "\n",
    "    feature_tmp_month_x = sin_transformer(12).fit_transform(feature_tmp_month)\n",
    "    feature_tmp_month_y = cos_transformer(12).fit_transform(feature_tmp_month)\n",
    "\n",
    "    feature_tmp_hour_x = sin_transformer(60).fit_transform(feature_tmp_hour)\n",
    "    feature_tmp_hour_y = cos_transformer(60).fit_transform(feature_tmp_hour)\n",
    "\n",
    "    feature_tmp_minute_x = sin_transformer(60).fit_transform(feature_tmp_minute)\n",
    "    feature_tmp_minute_y = cos_transformer(60).fit_transform(feature_tmp_minute)\n",
    "\n",
    "    feature_tmp_second_x = sin_transformer(60).fit_transform(feature_tmp_second)\n",
    "    feature_tmp_second_y = cos_transformer(60).fit_transform(feature_tmp_second)\n",
    "\n",
    "    feature_tmp_day_of_week_x = sin_transformer(7).fit_transform(feature_tmp_day_of_week)\n",
    "    feature_tmp_day_of_week_y = cos_transformer(7).fit_transform(feature_tmp_day_of_week)\n",
    "\n",
    "    feature_tmp = [feature_tmp_year, \n",
    "                    feature_tmp_month_x,\n",
    "                    feature_tmp_month_y,\n",
    "                    feature_tmp_day,\n",
    "                    feature_tmp_hour_x,\n",
    "                    feature_tmp_hour_y,\n",
    "                    feature_tmp_minute_x,\n",
    "                    feature_tmp_minute_y,\n",
    "                    feature_tmp_second_x,\n",
    "                    feature_tmp_second_y,\n",
    "                    feature_tmp_day_of_week_x,\n",
    "                    feature_tmp_day_of_week_y,\n",
    "                    ]\n",
    "    return np.vstack(feature_tmp).T\n",
    "\n",
    "def auto_enc(df):\n",
    "    mapping = dict()\n",
    "    for i, _ in enumerate(df.columns):\n",
    "        if df.dtypes.iloc[i] == \"float64\":\n",
    "            mapping[str(df.columns[i])] = \"proc_raw\"\n",
    "        elif df.dtypes.iloc[i] == \"object\":\n",
    "            if (len(df[df.columns[i]].unique()) / len(df) <= THRESHOLD_RATIO_CATEGORIES) & \\\n",
    "                (len(df[df.columns[i]].unique()) <= THRESHOLD_ABSOLUTE_CATEGORIES):\n",
    "                mapping[str(df.columns[i])] = \"proc_objects_one_hot\"\n",
    "            else:\n",
    "                mapping[str(df.columns[i])] = \"proc_objects_string\"\n",
    "        elif df.dtypes.iloc[i] == \"int64\":\n",
    "            if (len(df[df.columns[i]].unique()) / len(df) <= THRESHOLD_RATIO_CATEGORIES) & \\\n",
    "                (len(df[df.columns[i]].unique()) <= THRESHOLD_ABSOLUTE_CATEGORIES):\n",
    "                mapping[str(df.columns[i])] = \"proc_objects_one_hot\"\n",
    "            else:\n",
    "                mapping[str(df.columns[i])] = \"proc_raw\"\n",
    "        elif df.dtypes.iloc[i] == \"datetime64[ns]\":\n",
    "            #features.append( proc_datetime( df[df.columns[i]] ) )\n",
    "            mapping[str(df.columns[i])] = \"proc_datetime\"\n",
    "        else:\n",
    "            print(\"WARNING: UNKNOWN COLUMN TYPE\")\n",
    "        \n",
    "    return mapping\n",
    "\n",
    "\n",
    "def auto_edgerizer(dfs):\n",
    "    strong_edges = list()\n",
    "    all_maps = dict()\n",
    "    for name, df in dfs:\n",
    "        curr_map = dict()\n",
    "        for i in range(len(df.columns)):\n",
    "            if (df.dtypes.iloc[i] == \"object\") | (df.dtypes.iloc[i] == \"int64\"):\n",
    "                curr_map[df.columns[i]] = df[df.columns[i]].unique()\n",
    "        all_maps[str(name)] = copy.copy(curr_map)\n",
    "    \n",
    "    # Now find overlaps\n",
    "    keys = sorted(list(all_maps.keys()))\n",
    "    for k in range(len(keys)):\n",
    "        for l in range(len(keys)):\n",
    "            if k < l:\n",
    "                k_cols = sorted(list(all_maps[keys[k]].keys()))\n",
    "                l_cols = sorted(list(all_maps[keys[l]].keys()))\n",
    "                for kk in range(len(k_cols)):\n",
    "                    for ll in range(len(l_cols)):\n",
    "                        inter = set(all_maps[keys[k]][k_cols[kk]].tolist()).intersection( \n",
    "                            set(all_maps[keys[l]][l_cols[ll]].tolist() ))\n",
    "                        if (len(set(all_maps[keys[k]][k_cols[kk]].tolist() )) == len(inter)) | \\\n",
    "                            (len(set(all_maps[keys[l]][l_cols[ll]].tolist() )) == len(inter)):\n",
    "                            if len(set(all_maps[keys[l]][l_cols[ll]].tolist() )) == len(set(all_maps[keys[k]][k_cols[kk]].tolist() )) and \\\n",
    "                                len(set(all_maps[keys[l]][l_cols[ll]].tolist() )) == len(inter):\n",
    "                                tmp_dict = dict()\n",
    "                                tmp_dict[\"name\"] = str(keys[k]) + \"_\" + str(keys[l]) \n",
    "                                tmp_dict[\"from\"] = str(keys[k])\n",
    "                                tmp_dict[\"to\"] = str(keys[l])\n",
    "                                tmp_dict[\"transform\"] = \"\"\n",
    "                                tmp_dict[\"from_col\"] = str(k_cols[kk])\n",
    "                                tmp_dict[\"to_col\"] = str(l_cols[ll])\n",
    "                                strong_edges.append(tmp_dict)\n",
    "    return strong_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'district_id': 'proc_raw',\n",
       " 'A2': 'proc_objects_string',\n",
       " 'A3': 'proc_objects_one_hot',\n",
       " 'A4': 'proc_raw',\n",
       " 'A5': 'proc_raw',\n",
       " 'A6': 'proc_raw',\n",
       " 'A7': 'proc_raw',\n",
       " 'A8': 'proc_objects_one_hot',\n",
       " 'A9': 'proc_objects_one_hot',\n",
       " 'A10': 'proc_raw',\n",
       " 'A11': 'proc_raw',\n",
       " 'A12': 'proc_raw',\n",
       " 'A13': 'proc_raw',\n",
       " 'A14': 'proc_raw',\n",
       " 'A15': 'proc_raw',\n",
       " 'A16': 'proc_raw'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_district = auto_enc(district)\n",
    "features_district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_auto_config_from_dataframes(dfs, file=\"default01.yml\"):\n",
    "    \"\"\"\n",
    "    Create a yaml file as well as a python file to give a \n",
    "    customizable basis for any following pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Create Node-FeatureExtraction Mapping\n",
    "    mappings = dict()\n",
    "    for name, df in dfs:\n",
    "        mappings[name] = dict()\n",
    "        mappings[name][\"transform\"] = auto_enc(df)\n",
    "\n",
    "    # 2. Create Edge Mapping\n",
    "    strong_edges = auto_edgerizer(dfs)\n",
    "\n",
    "    # 3. Write script.py\n",
    "\n",
    "    final_dict = dict()\n",
    "    final_dict[\"nodes\"] = mappings\n",
    "    final_dict[\"edges\"] = strong_edges\n",
    "\n",
    "    # 4. Write yaml file\n",
    "    f = open(file, \"w\")\n",
    "    yaml.dump(final_dict, f, sort_keys=False)\n",
    "    f.close()\n",
    "    print(\"YAML file saved.\")\n",
    "    #print(final_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file saved.\n"
     ]
    }
   ],
   "source": [
    "create_auto_config_from_dataframes([\n",
    "        (\"trans\", trans),\n",
    "        (\"loan\", loan),\n",
    "        (\"order\", order),\n",
    "        (\"card\", card),\n",
    "        (\"account\", account),\n",
    "        (\"client\", client),\n",
    "        (\"disp\", disp),\n",
    "        (\"district\", district)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_pipeline_from_yaml(file=\"../default.yml\"):\n",
    "    config = load_yaml(file)\n",
    "    return config\n",
    "\n",
    "#run_feature_pipeline_from_yaml()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Dataframes Schema\n",
    "# 1. Read in the Transformation pipelines etc\n",
    "# 2. Create Batch\n",
    "# 3. Load model checkpoint and evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgnn_demos-1xirZSoL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
