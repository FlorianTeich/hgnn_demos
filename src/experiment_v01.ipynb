{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment v01 -\n",
    "## Classic ML vs Hetero-GNN-Benchmark - POC\n",
    "\n",
    "* Data:\n",
    "    * Synthetic Data will be generated about 1000 people, and 10000 diagnoses related to them\n",
    "    * People will get assigned labels depending on whether they had a specific diagnosis or not\n",
    "\n",
    "* Split:\n",
    "    * Data will be split 60:20:20 int train, val and test splits\n",
    "\n",
    "* Classic ML vs GNN:\n",
    "    * GradientBoosting Model will be trained on peoples properties to predict the label\n",
    "        * Features: age, gender, diagnoses (one-hot encoded)\n",
    "    * HGNN:\n",
    "        * Features people: age\n",
    "        * Features diagnoses: age and icd code (one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import neo4j\n",
    "import time\n",
    "import string\n",
    "import datetime\n",
    "import testkasse\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from dotenv import load_dotenv\n",
    "from torch_geometric.data import Dataset, Batch\n",
    "from torch.nn import Linear, ReLU, Sequential, LeakyReLU\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import HGTLoader, NeighborLoader, DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    SAGEConv,\n",
    "    to_hetero,\n",
    "    to_hetero_with_bases,\n",
    "    global_max_pool,\n",
    "    MeanAggregation,\n",
    "    MinAggregation,\n",
    "    GraphConv,\n",
    "    MaxAggregation,\n",
    ")\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch_geometric.nn import BatchNorm, GraphConv\n",
    "from torch_geometric.nn import MultiAggregation\n",
    "\n",
    "load_dotenv()\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from utils_neo4j import *\n",
    "from utils_middleware_v1 import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = driver.session()\n",
    "result = session.run(\"MATCH (v)-[r]->(d) RETURN v,r,d LIMIT 20\")\n",
    "w = GraphWidget(graph=result.graph())\n",
    "w.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples - Versicherte & Diagnosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10_000\n",
    "\n",
    "backend = sqlalchemy.create_engine(\"sqlite:///test.db\")\n",
    "\n",
    "testkasse.generate_all_tables(anzahl_versicherte=NUM_SAMPLES, conn=backend)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stammdaten = pd.read_sql_table(\"stammdaten\", con=backend)\n",
    "icd_data = pd.read_sql_table(\"kh\", con=backend)\n",
    "\n",
    "train, validate, test = np.split(\n",
    "    stammdaten.sample(frac=1), [int(0.6 * len(stammdaten)), int(0.8 * len(stammdaten))]\n",
    ")\n",
    "stammdaten.loc[train.index, \"mode\"] = \"train\"\n",
    "stammdaten.loc[validate.index, \"mode\"] = \"validate\"\n",
    "stammdaten.loc[test.index, \"mode\"] = \"test\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering to make data compatible with Classic ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Short ICD Codes\n",
    "icd_data[\"icd_short\"] = icd_data[\"ICD\"].str[:1]\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "icd_data = pd.read_sql_table(\"kh\", con=backend)\n",
    "\n",
    "# Get Geschlecht und Alter\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(stammdaten[stammdaten[\"mode\"] != \"test\"].Geschlecht)\n",
    "stammdaten[\"geschlecht_encoded\"] = le.transform(stammdaten.Geschlecht)\n",
    "\n",
    "stammdaten[\"now\"] = pd.to_datetime(datetime.datetime.now())\n",
    "stammdaten[\"alter\"] = (stammdaten[[\"Todestag\", \"now\"]].min(axis=1) - stammdaten.Geburtstag).dt.days\n",
    "stammdaten[\"alter\"] = scaler2.fit_transform(np.expand_dims(pd.to_numeric(stammdaten[\"alter\"], downcast=\"float\"), 1))\n",
    "\n",
    "# Get icd codes\n",
    "scaler = MaxAbsScaler()\n",
    "icd_data = icd_data.merge(stammdaten[[\"KVNR\", \"Geburtstag\", \"mode\"]], on=\"KVNR\", how=\"left\")\n",
    "icd_data[\"icd_age\"] = (icd_data[\"kh_von\"] - icd_data.Geburtstag).dt.days\n",
    "scaler.fit(icd_data[icd_data[\"mode\"] != \"test\"][[\"icd_age\"]])\n",
    "icd_data[\"icd_age\"] = scaler.transform(icd_data[[\"icd_age\"]])\n",
    "icd_data[\"icd_short\"] = icd_data[\"ICD\"].str[:1].apply(str.lower)\n",
    "icd_agg = icd_data.groupby(\"KVNR\")[\"icd_short\"].apply(\" \".join)\n",
    "\n",
    "merged = stammdaten.merge(icd_agg, how=\"left\", on=\"KVNR\")\n",
    "\n",
    "\n",
    "def analyzer_custom(doc):\n",
    "    return doc.split()\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=list(string.ascii_lowercase), analyzer=analyzer_custom)\n",
    "merged[\"icd_short\"] = merged[\"icd_short\"].fillna(\"a\")\n",
    "data = vectorizer.transform(merged[\"icd_short\"]).todense()\n",
    "merged.loc[:, \"icd_vec\"] = data.tolist()\n",
    "X = np.hstack([merged[[\"geschlecht_encoded\", \"alter\"]].values, np.vstack(merged[\"icd_vec\"].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add Min, Max, Mean and Sum of ages of each ICD to the featurelist of the GradientBoosting Method to provide it with more relevant info\n",
    "# def prop_to_vec(val):\n",
    "#    x = np.zeros((1, 26))\n",
    "#    x[0, ord(val) - 97] = 1\n",
    "#    return x\n",
    "\n",
    "# icd_data[\"inds\"] = icd_data[\"ICD\"].str[:1].apply(str.lower).apply(prop_to_vec)\n",
    "# icd_data[\"inds\"] = icd_data[\"inds\"] * icd_data.icd_age\n",
    "# icd_data[\"inds\"] = np.zeros((10000, 26)).tolist()\n",
    "# icd_data[\"min_vecs\"] = icd_data[[\"KVNR\", \"inds\"]].groupby(\"KVNR\").max()\n",
    "# icd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# icd_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Label: Was diagnosed very early with specific Diagnosis\n",
    "icd_data[\"label\"] = (icd_data[\"icd_short\"] <= \"d\") & (icd_data[\"icd_age\"] < 0)\n",
    "stammdaten = stammdaten.merge(icd_data[[\"KVNR\", \"label\"]].groupby(\"KVNR\").max(), on=\"KVNR\", how=\"left\")\n",
    "stammdaten[\"label\"] = stammdaten[\"label\"].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stammdaten.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic ML Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train classic ML model (XGBoost)\n",
    "train_inds = stammdaten[stammdaten[\"mode\"] == \"train\"].index.tolist()\n",
    "val_inds = stammdaten[stammdaten[\"mode\"] == \"validate\"].index.tolist()\n",
    "test_inds = stammdaten[stammdaten[\"mode\"] == \"test\"].index.tolist()\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(\n",
    "    X[train_inds], stammdaten.loc[train_inds, [\"label\"]].astype(int).values.ravel()\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Accuracy Test Daten, GradientBoosting:\",\n",
    "    clf.score(X[val_inds], stammdaten.loc[val_inds, [\"label\"]].astype(int).values.ravel()),\n",
    ")\n",
    "\n",
    "print(\"Features: Geschlecht, Alter, ICD-One-Hot-encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\"feature_\" + str(i) for i in range(X.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(data=X[train_inds].tolist(), columns=feats),\n",
    "        pd.DataFrame({\"label\": stammdaten.loc[train_inds, [\"label\"]].astype(int).values.ravel()}),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df_test = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(data=X[test_inds].tolist(), columns=feats),\n",
    "        pd.DataFrame({\"label\": stammdaten.loc[test_inds, [\"label\"]].astype(int).values.ravel()}),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "train_data = TabularDataset(df_train)\n",
    "test_data = TabularDataset(df_test)\n",
    "\n",
    "predictor = TabularPredictor(label=\"label\").fit(train_data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = TabularPredictor.load(\"AutogluonModels/ag-20230831_124650/\")\n",
    "predictions = predictor.predict(test_data)\n",
    "\n",
    "import sklearn\n",
    "\n",
    "sklearn.metrics.accuracy_score(predictions, test_data.label.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Data to neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.query(\"DROP CONSTRAINT DIAGNOSE\")\n",
    "conn.query(\"CREATE CONSTRAINT versicherter IF NOT EXISTS FOR (v:versicherter) REQUIRE v.KVNR IS UNIQUE\")\n",
    "conn.query(\"MATCH (n) DETACH DELETE n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_diagnosen_v1(icd_data[[\"ICD\", \"icd_age\", \"icd_short\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_versicherte_v1(stammdaten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.query(\"DROP CONSTRAINT DIAGNOSE\")\n",
    "conn.query(\"CREATE CONSTRAINT versicherter IF NOT EXISTS FOR (v:versicherter) REQUIRE v.KVNR IS UNIQUE\")\n",
    "conn.query(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "add_diagnosen_v1(icd_data[[\"ICD\", \"icd_age\", \"icd_short\"]])\n",
    "add_versicherte_v1(stammdaten)\n",
    "add_relations_v1(icd_data.merge(stammdaten, on=\"KVNR\"))\n",
    "\n",
    "# conn.query(\"MATCH (v: versicherter {KVNR: '\" + icd_data.iloc[1].KVNR + \"'})-[:DIAGNOSTIZIERT]-(d) RETURN v, d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader-Helper-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_graph(idx=0):\n",
    "    versicherter_query = (\n",
    "        \"\"\"\n",
    "    MATCH (v:versicherter {id: '\"\"\"\n",
    "        + stammdaten.iloc[idx].KVNR\n",
    "        + \"\"\"'})-[:DIAGNOSTIZIERT]->(d:diagnose)\n",
    "    WITH v, collect(d.name) AS diagnose_list\n",
    "    RETURN v.id AS versichertenId, v.alter AS alter, v.KVNR AS KVNR, v.label AS label\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    versicherter_x_, versicherter_mapping_ = load_node(\n",
    "        versicherter_query,\n",
    "        index_col=\"versichertenId\",\n",
    "        encoders={\n",
    "            \"alter\": ScalarIdentityEncoder(torch.float),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    diagnose_query = (\n",
    "        \"\"\"\n",
    "    MATCH (v:versicherter {id: '\"\"\"\n",
    "        + stammdaten.iloc[idx].KVNR\n",
    "        + \"\"\"'})-[:DIAGNOSTIZIERT]->(d:diagnose)\n",
    "    WITH d\n",
    "    RETURN d.name AS diagnoseId, d.name AS name, d.icd_age AS icd_age, d.icd_short AS icd_short\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    diagnose_x_, diagnose_mapping_ = load_node(\n",
    "        diagnose_query,\n",
    "        index_col=\"diagnoseId\",\n",
    "        encoders={\"icd_short\": IcdEncoder(), \"icd_age\": ScalarIdentityEncoder()},\n",
    "    )\n",
    "\n",
    "    edge_query = (\n",
    "        \"\"\"\n",
    "    MATCH (v:versicherter {id: '\"\"\"\n",
    "        + stammdaten.iloc[idx].KVNR\n",
    "        + \"\"\"'})-[r:DIAGNOSTIZIERT]->(d:diagnose) \n",
    "    RETURN v.id AS vId, d.name AS dId\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    edge_index_, edge_label_ = load_edge(\n",
    "        edge_query,\n",
    "        src_index_col=\"vId\",\n",
    "        src_mapping=versicherter_mapping_,\n",
    "        dst_index_col=\"dId\",\n",
    "        dst_mapping=diagnose_mapping_,\n",
    "    )\n",
    "\n",
    "    data_ = HeteroData()\n",
    "    data_[\"versicherter\"].x = versicherter_x_\n",
    "    data_[\"versicherter\"].label = torch.from_numpy(np.array(int(stammdaten.iloc[idx].label)))\n",
    "    data_[\"diagnose\"].x = diagnose_x_.float()\n",
    "    data_[\"versicherter\", \"hat\", \"diagnose\"].edge_index = edge_index_\n",
    "    data_ = ToUndirected()(data_)\n",
    "    data_.to(device, non_blocking=True)\n",
    "    return data_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOwnDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None, mode=\"train\"):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.mode = mode\n",
    "        if mode == \"train\":\n",
    "            self.inds = train.index.tolist()\n",
    "        elif mode == \"val\":\n",
    "            self.inds = validate.index.tolist()\n",
    "        elif mode == \"test\":\n",
    "            self.inds = test.index.tolist()\n",
    "        # print(self.inds)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\n",
    "            \"some_file_1\",\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\n",
    "            \"data_1.pt\",\n",
    "        ]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.inds)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = get_single_graph(self.inds[idx])\n",
    "        return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = MyOwnDataset(root=\"\", mode=\"train\")\n",
    "dset_val = MyOwnDataset(root=\"\", mode=\"val\")\n",
    "dset_test = MyOwnDataset(root=\"\", mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from collections import OrderedDict\n",
    "\n",
    "batch_size = 32\n",
    "hidden_size = 4\n",
    "num_classes = 2\n",
    "learn_rate = 0.01\n",
    "aggr=\"max\"\n",
    "\n",
    "\n",
    "class GraphLevelGNN(torch.nn.Module):\n",
    "    def __init__(self, config={\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"activation\": F.leaky_relu,\n",
    "            \"batchnorm\": BatchNorm,\n",
    "            \"enable_dropout\": False,\n",
    "            \"graphlayer\": GraphConv,\n",
    "            \"num_graphlayers\": 2}):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.body = F.nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        #self.conv1 = GraphConv(-1, hidden_size)\n",
    "        #self.bn1 = BatchNorm(hidden_size)\n",
    "        self.pool = MultiAggregation(\n",
    "            aggrs=['mean', 'min', 'max'],\n",
    "            mode=\"cat\"\n",
    "            )\n",
    "        self.lin = Linear(hidden_size*3, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, batch: Tensor) -> Tensor:\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "g1 = get_single_graph(647)\n",
    "g2 = get_single_graph(1)\n",
    "btch = Batch.from_data_list([g1, g2])\n",
    "\n",
    "metadata = g1.metadata()\n",
    "\n",
    "model = GraphLevelGNN()\n",
    "model = to_hetero(model, metadata, aggr=aggr, debug=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "out = model(btch.x_dict, btch.edge_index_dict, btch.batch_dict)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = dset_train.get(0)\n",
    "g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "\n",
    "\n",
    "g3 = T.AddSelfLoops()(g1)\n",
    "g3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OGB_MAG(root='./data', preprocess='metapath2vec', transform=T.ToUndirected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear, DeepGCNLayer\n",
    "\n",
    "batch_size = 32\n",
    "hidden_size = 27\n",
    "num_classes = 2\n",
    "learn_rate = 0.01\n",
    "aggr = \"max\"\n",
    "\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "\n",
    "# dataset = OGB_MAG(root='./data', preprocess='metapath2vec', transform=T.ToUndirected())\n",
    "data = dset_train[0]\n",
    "\n",
    "\n",
    "class GraphLevelGNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convX = GraphConv(-1, 32)\n",
    "        self.pool = MultiAggregation(aggrs=[\"mean\", \"min\", \"max\"], mode=\"cat\")\n",
    "        self.lin = Linear(hidden_size * 3, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, batch: Tensor) -> Tensor:\n",
    "        x = self.convX(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "\n",
    "class GraphLevelGNNRes(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(-1, 27)\n",
    "        self.conv2 = GraphConv(-1, 27)\n",
    "        self.conv3 = GraphConv(-1, 27)\n",
    "        self.batchnorm1 = BatchNorm(27)\n",
    "        self.pool = MultiAggregation(aggrs=[\"mean\", \"min\", \"max\"], mode=\"cat\")\n",
    "        self.lin = Linear(hidden_size * 3, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, batch: Tensor) -> Tensor:\n",
    "        # h = checkpoint(self.conv1, x)\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h1 = self.batchnorm1(h1)\n",
    "        h1 = F.leaky_relu(h1)\n",
    "        h1 = x + h1\n",
    "        h1 = F.dropout(h1, p=0.1, training=self.training)\n",
    "\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h2 = self.batchnorm1(h2)\n",
    "        h2 = F.leaky_relu(h2)\n",
    "        h2 = h1 + h2\n",
    "        h2 = F.dropout(h2, p=0.1, training=self.training)\n",
    "\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "        h3 = self.batchnorm1(h3)\n",
    "        h3 = F.leaky_relu(h3)\n",
    "        h3 = h2 + h3\n",
    "        h3 = F.dropout(h3, p=0.1, training=self.training)\n",
    "\n",
    "        h3 = self.pool(h3, batch)\n",
    "        h3 = self.lin(h3)\n",
    "        return h3\n",
    "\n",
    "\n",
    "g1 = get_single_graph(647)\n",
    "g2 = get_single_graph(1)\n",
    "\n",
    "btch = Batch.from_data_list([g1, g2])\n",
    "\n",
    "metadata = g1.metadata()\n",
    "\n",
    "model = GraphLevelGNNRes()\n",
    "model = to_hetero(model, metadata, aggr=aggr, debug=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "out = model(btch.x_dict, btch.edge_index_dict, btch.batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "writer = SummaryWriter(\"logs/eperiment-01-single_graphconv\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dset_train, batch_size=32, shuffle=True, num_workers=12)\n",
    "val_loader = DataLoader(dset_val, batch_size=16, shuffle=False, num_workers=12)\n",
    "test_loader = DataLoader(dset_test, batch_size=16, shuffle=False, num_workers=12)\n",
    "\n",
    "# writer.add_hparams({\n",
    "#    \"train_batch_size\": batch_size\n",
    "# })\n",
    "\n",
    "\n",
    "def train(epoch=0):\n",
    "    model.train()\n",
    "    preds = []\n",
    "    ytrue = []\n",
    "    all_loss = 0.0\n",
    "    for btch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(btch.x_dict, btch.edge_index_dict, btch.batch_dict)\n",
    "        loss = F.cross_entropy(out, btch[\"versicherter\"].label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds.extend(torch.argmax(torch.softmax(out, dim=-1), dim=-1).detach().tolist())\n",
    "        ytrue.extend(btch[\"versicherter\"].label.tolist())\n",
    "        all_loss += loss.item()\n",
    "    acc = accuracy_score(ytrue, preds)\n",
    "    print(\"Train Loss:\", all_loss)\n",
    "    print(\"Train Acc:\", acc)\n",
    "    writer.add_scalar(\"Loss/train\", all_loss, epoch)\n",
    "    writer.add_scalar(\"Acc/train\", acc, epoch)\n",
    "    all_loss = 0.0\n",
    "    preds = []\n",
    "    ytrue = []\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch=0):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    ytrue = []\n",
    "    all_loss = 0.0\n",
    "    for btch in tqdm(test_loader):\n",
    "        out = model(btch.x_dict, btch.edge_index_dict, btch.batch_dict)\n",
    "        loss = F.cross_entropy(out, btch[\"versicherter\"].label)\n",
    "        preds.extend(torch.argmax(torch.softmax(out, dim=-1), dim=-1).detach().tolist())\n",
    "        ytrue.extend(btch[\"versicherter\"].label.tolist())\n",
    "        all_loss += loss.item()\n",
    "    acc = accuracy_score(ytrue, preds)\n",
    "    print(\"Test Loss:\", all_loss)\n",
    "    print(\"Test Acc:\", acc)\n",
    "    writer.add_scalar(\"Loss/test\", all_loss, epoch)\n",
    "    writer.add_scalar(\"Acc/test\", acc, epoch)\n",
    "    all_loss = 0.0\n",
    "    preds = []\n",
    "    ytrue = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    train(i)\n",
    "    test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc-gnn-q9HPy68v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
